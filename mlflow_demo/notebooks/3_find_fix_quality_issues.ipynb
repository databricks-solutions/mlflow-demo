{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find & Fix Quality Issues\n",
    "\n",
    "## Systematic workflow for discovering problems, creating solutions, and deploying improvements safely\n",
    "\n",
    "Learn to systematically find and fix quality issues in your GenAI application using a real-world scenario. In this hands-on workflow, we'll address accuracy problems where the email generator is \"hallucinating\" information not present in the customer data.\n",
    "\n",
    "## The Problem Scenario\n",
    "\n",
    "Your email generation system has been running in production, but users report accuracy issues. Some emails contain fabricated details about:\n",
    "\n",
    "- Product features not mentioned in customer data\n",
    "- Meeting discussions that didn't happen as described\n",
    "- Support ticket information with incorrect details\n",
    "\n",
    "## The Solution Workflow\n",
    "\n",
    "Follow this systematic 6-step process to identify, fix, and safely deploy improvements:\n",
    "\n",
    "1. **üîç Discover quality issues** Find problematic traces using evaluation results\n",
    "2. **üìä Create Evaluation Datasets** Turn bad traces into targeted evaluation sets, good traces into regression sets\n",
    "3. **üéØ Iterate on changes** Use MLflow Prompt Registry to track your changes\n",
    "4. **üß™ Evaluate changes improved quality** Test that your changes addressed the quality problems\n",
    "5. **üõ°Ô∏è Verify changes didn't cause a regression** Ensure fixes don't break user inputs that already work well\n",
    "6. **üöÄ Deploy**\n",
    "\n",
    "This approach ensures evidence-based improvements rather than guesswork, with quantitative validation and safe deployment practices.\n",
    "\n",
    "![test-new-prompt](https://i.imgur.com/4wlhT63.gif)\n",
    "\n",
    "This notebook provides hands-on experience with MLflow's complete quality improvement workflow for GenAI applications. You'll learn to systematically identify quality issues in production traces, create evaluation datasets, test improved prompts, and make data-driven deployment decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages (only required if running in a Databricks Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -r ../../requirements.txt\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Load environment variables and verify MLflow configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import mlflow\n",
    "from mlflow_demo.utils import *\n",
    "\n",
    "if mlflow.utils.databricks_utils.is_in_databricks_notebook():\n",
    "  print(\"Running in Databricks Notebook\")\n",
    "  setup_databricks_notebook_env()\n",
    "else:\n",
    "  print(\"Running in Local IDE\")\n",
    "  setup_local_ide_env()\n",
    "\n",
    "# Verify key variables are loaded\n",
    "print('=== Environment Setup ===')\n",
    "print(f'DATABRICKS_HOST: {os.getenv(\"DATABRICKS_HOST\")}')\n",
    "print(f'MLFLOW_EXPERIMENT_ID: {os.getenv(\"MLFLOW_EXPERIMENT_ID\")}')\n",
    "print(f'LLM_MODEL: {os.getenv(\"LLM_MODEL\")}')\n",
    "print(f'UC_CATALOG: {os.getenv(\"UC_CATALOG\")}')\n",
    "print(f'UC_SCHEMA: {os.getenv(\"UC_SCHEMA\")}')\n",
    "print('‚úÖ Environment variables loaded successfully!')\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get helper functions for showing links to generated traces\n",
    "from mlflow_demo.utils import generate_trace_links, generate_dataset_link, generate_prompt_link, generate_evaluation_comparison_link, generate_evaluation_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Step 1: Discover Quality Issues in Production Traces\n",
    "\n",
    "Let's start by examining real production traces to identify quality problems. Once you have tracing in place and judges defined, you can systematically improve your GenAI application by analyzing where it's failing.\n",
    "\n",
    "Your email generation system has been running in production, but users report accuracy issues. Some emails contain fabricated details about:\n",
    "\n",
    "- Product features not mentioned in customer data\n",
    "- Meeting discussions that didn't happen as described\n",
    "- Support ticket information with incorrect details\n",
    "\n",
    "Here, we will:\n",
    "\n",
    "1. Use the `accuracy` scorer that we created in the previous tutorial to identify problematic traces we need to fix\n",
    "2. Use the `accuracy`, `personalized`, and `relevance` scorers to identify traces that are performing well so we can ensure our change doesn't regress quality\n",
    "\n",
    "### **IMPORTANT**: In this notebook, we show you how to find traces using the MLflow SDKs. You can also perform these same steps using the MLflow Experiment UI - see the UI animation at the top of this notebook!\n",
    "\n",
    "**üìö Documentation**\n",
    "\n",
    "- [**Build Evaluation Datasets**](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/build-eval-dataset) - Create datasets from production traces\n",
    "- [**Search and Filter Traces**](https://docs.databricks.com/aws/en/mlflow3/genai/tracing/manage-traces) - Find specific traces by criteria\n",
    "\n",
    "**‚ñ∂Ô∏è Run the next cells to search for traces with quality issues!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "print('üîç Searching for production traces with evaluation results...')\n",
    "# Get 50 traces from  from the current experiment\n",
    "# The tag is used to identify the traces that the demo initially loaded - think of this as your production set of traces from real users.\n",
    "traces = mlflow.search_traces(max_results=50, filter_string='tags.sample_data = \"yes\"')\n",
    "\n",
    "print(f'üìä Found {len(traces)} total traces')\n",
    "\n",
    "# Initialize lists to collect trace data\n",
    "failed_accuracy_traces_data = []\n",
    "high_quality_traces_data = []\n",
    "\n",
    "print('\\nüîç Analyzing trace assessments for quality issues...')\n",
    "\n",
    "# Iterate through DataFrame rows\n",
    "for idx, trace_row in traces.iterrows():\n",
    "\n",
    "  # Access trace info\n",
    "  trace_info = trace_row.info\n",
    "\n",
    "  # Get assessments - adapt based on actual data structure\n",
    "  assessments = trace_row.assessments\n",
    "  assessments_count = len(assessments)\n",
    "\n",
    "\n",
    "  # Get trace_id - adapt based on actual data structure\n",
    "  trace_id = getattr(trace_info, 'trace_id', None)\n",
    "  if trace_id is None and 'trace_id' in trace_row:\n",
    "    trace_id = trace_row['trace_id']\n",
    "\n",
    "  if assessments_count == 0:\n",
    "    if trace_id:\n",
    "      print(f'‚ö†Ô∏è  No assessments found for trace {trace_id[:8]}... (skipping)')\n",
    "    continue\n",
    "\n",
    "  # Track the number of passed assessments for this trace\n",
    "  passed_assessments = 0\n",
    "  failed_accuracy = False\n",
    "\n",
    "  for assessment in assessments:\n",
    "    # Get the scorer's assessments\n",
    "    assessment_name = assessment.get('assessment_name')\n",
    "    assessment_feedback = assessment.get('feedback', {}).get('value')\n",
    "\n",
    "    if assessment_name == 'accuracy' and assessment_feedback == 'no':\n",
    "      failed_accuracy = True\n",
    "    elif (\n",
    "      assessment_name in ['relevance', 'personalized', 'accuracy']\n",
    "      and assessment_feedback == 'yes'\n",
    "      ):\n",
    "        passed_assessments += 1\n",
    "\n",
    "  # Categorize traces based on quality and store full trace data, up to 5 of each\n",
    "  if failed_accuracy and len(failed_accuracy_traces_data) < 5:\n",
    "    failed_accuracy_traces_data.append(trace_row)\n",
    "    if trace_id:\n",
    "      _, trace_url = generate_trace_links(trace_id, print_urls=False)\n",
    "      print(f'‚ùå Failed accuracy: {trace_id[:8]}... (view trace: {trace_url})')\n",
    "  elif passed_assessments >= 3 and len(high_quality_traces_data) < 5:\n",
    "    high_quality_traces_data.append(trace_row)\n",
    "    if trace_id:\n",
    "      _, trace_url = generate_trace_links(trace_id, print_urls=False)\n",
    "      print(\n",
    "        f'‚úÖ High quality: {trace_id[:8]}... (passed {passed_assessments} assessments, view trace: {trace_url})'\n",
    "      )\n",
    "\n",
    "# Create DataFrames with same structure as search_traces(...) - this is required to create Evaluation Datasets in the next step\n",
    "print('\\nüíæ Saving traces as DataFrames for dataset creation...')\n",
    "failed_accuracy_traces = pd.DataFrame(failed_accuracy_traces_data)\n",
    "high_quality_traces = pd.DataFrame(high_quality_traces_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Step 2: Create Evaluation and Regression Datasets\n",
    "\n",
    "Now that we've identified problematic traces, let's create two Evaluation Datasets. An MLflow Evaluation Dataset is an MLflow primitive that allows you to curate collections of traces to use for evaluation. Evaluation Datasets are stored as Delta Tables in Unity Catalog.\n",
    "\n",
    "1. **Dataset with problematic traces**: Contains traces with accuracy issues (to test our improvements)\n",
    "2. **Regression dataset with high-quality traces**: Contains high-quality traces (to ensure we don't break existing functionality)\n",
    "\n",
    "These datasets enable systematic testing of prompt improvements and ensure we don't introduce new problems while fixing existing ones.\n",
    "\n",
    "**üìö Documentation**\n",
    "\n",
    "- [**Create Evaluation Datasets**](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/build-eval-dataset) - Build datasets from traces\n",
    "- [**Dataset Management**](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/build-eval-dataset#manage-datasets) - Manage evaluation datasets\n",
    "\n",
    "**‚ñ∂Ô∏è Run the next cells to create these datasets!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai import datasets\n",
    "from datetime import datetime\n",
    "# Dataset locations (uses the same Unity Catalog from the initial setup - these can be changed if needed)\n",
    "UC_CATALOG = os.getenv('UC_CATALOG', 'default_catalog')\n",
    "UC_SCHEMA = os.getenv('UC_SCHEMA', 'default_schema')\n",
    "\n",
    "# Use unique names for datasets to avoid conflicts - you can change these if desired\n",
    "LOW_ACCURACY_DATASET_NAME = f'low_accuracy_dataset_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "REGRESSION_DATASET_NAME = f'regression_dataset_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "\n",
    "print(f'\\nüìä Creating datasets in {UC_CATALOG}.{UC_SCHEMA}...')\n",
    "\n",
    "# Create low accuracy dataset - this will fail if you've already created a dataset with this name\n",
    "low_accuracy_dataset = datasets.create_dataset(\n",
    "    uc_table_name=f'{UC_CATALOG}.{UC_SCHEMA}.{LOW_ACCURACY_DATASET_NAME}',\n",
    ")\n",
    "print(f'‚úÖ Created new low accuracy dataset: {UC_CATALOG}.{UC_SCHEMA}.{LOW_ACCURACY_DATASET_NAME}')\n",
    "\n",
    "# Add traces from step 1 with accuracy issues to the low accuracy dataset\n",
    "low_accuracy_dataset.merge_records(failed_accuracy_traces)\n",
    "print(f'üìù Added {len(failed_accuracy_traces)} records into low accuracy dataset')\n",
    "generate_dataset_link(low_accuracy_dataset.dataset_id)\n",
    "\n",
    "# Create regression dataset - this will fail if you've already created a dataset with this name\n",
    "regression_dataset = datasets.create_dataset(\n",
    "    uc_table_name=f'{UC_CATALOG}.{UC_SCHEMA}.{REGRESSION_DATASET_NAME}',\n",
    "    )\n",
    "print(f'‚úÖ Created new regression dataset: {UC_CATALOG}.{UC_SCHEMA}.{REGRESSION_DATASET_NAME}')\n",
    "\n",
    "# Add traces from step 1 with high quality to the regression dataset\n",
    "regression_dataset.merge_records(high_quality_traces)\n",
    "print(f'üìù Added {len(high_quality_traces)} records into regression dataset')\n",
    "generate_dataset_link(regression_dataset.dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Step 3: Investigate Quality Issues: Understanding What's Going Wrong\n",
    "\n",
    "Now let's dive deeper into the problematic traces to understand the root causes of our quality issues. By analyzing the judge rationales and specific examples, we can identify patterns in the failures and design targeted improvements.\n",
    "\n",
    "### **IMPORTANT**: In this notebook, we show you how to view this data using the MLflow SDKs. You can also perform these same steps using the MLflow Experiment UI:\n",
    "\n",
    "![investigate](https://i.imgur.com/DFN91Pu.gif)\n",
    "\n",
    "## What We'll Investigate\n",
    "\n",
    "**Judge Rationales Analysis**: Examine why the accuracy scorer flagged these traces as problematic\n",
    "\n",
    "- What specific fabrications or hallucinations occurred?\n",
    "- Which types of information are being invented (features, meetings, tickets)?\n",
    "- Are there patterns in the failure modes?\n",
    "\n",
    "**Input vs Output Comparison**: Compare customer data with generated emails\n",
    "\n",
    "- Identify where the model went beyond provided data\n",
    "- Find instances of invented product features or capabilities\n",
    "- Spot fabricated meeting details or support ticket information\n",
    "\n",
    "**Root Cause Analysis**: Determine the underlying prompt issues\n",
    "\n",
    "- Is the current prompt too vague about data constraints?\n",
    "- Are there missing guardrails against fabrication?\n",
    "- Does the prompt encourage creativity over accuracy?\n",
    "\n",
    "This investigation will inform our prompt improvements in the next step, ensuring we address the actual causes rather than symptoms.\n",
    "\n",
    "**üìö Documentation**\n",
    "\n",
    "- [**Trace Analysis**](https://docs.databricks.com/aws/en/mlflow3/genai/tracing/manage-traces) - Analyze production traces\n",
    "- [**Judge Feedback**](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/custom-scorers#judge-feedback) - Understanding evaluation rationales\n",
    "\n",
    "**‚ñ∂Ô∏è Click on the trace links above to examine specific failures in the MLflow UI, then continue to Step 2!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate failed traces and extract judge rationales\n",
    "print('üîç Investigating Failed Accuracy Traces:')\n",
    "print('=' * 80)\n",
    "\n",
    "for idx, failed_trace in failed_accuracy_traces.iterrows():\n",
    "    # Get trace_id - it's stored in the info object\n",
    "    trace_info = failed_trace.info\n",
    "    trace_id = getattr(trace_info, 'trace_id', None)\n",
    "\n",
    "    _, trace_url = generate_trace_links(trace_id, print_urls=False)\n",
    "    print(f'\\n‚ùå Trace ID: {trace_id[:8] if trace_id else \"Unknown\"}...')\n",
    "    print(f'   üîó View Trace: {trace_url}')\n",
    "\n",
    "    # Extract and display accuracy assessment rationale\n",
    "    assessments = failed_trace.assessments\n",
    "    for assessment in assessments:\n",
    "        assessment_name = assessment.get('assessment_name')\n",
    "        assessment_rationale = assessment.get('rationale')\n",
    "        assessment_feedback = assessment.get('feedback', {}).get('value')\n",
    "\n",
    "        if assessment_name == 'accuracy' and assessment_feedback == 'no':\n",
    "            # Print the judge's rationale for why this failed accuracy\n",
    "            print(f'   üìù Accuracy Judge Rationale:')\n",
    "            # Word wrap the rationale for better readability\n",
    "            import textwrap\n",
    "            wrapped_rationale = textwrap.fill(assessment_rationale, width=80, initial_indent='      ', subsequent_indent='      ')\n",
    "            print(wrapped_rationale)\n",
    "            break\n",
    "\n",
    "\n",
    "print('\\nüí° Patterns we noticed when reviewing these rationales:')\n",
    "print('   üö´ Fabricated product features not in customer data')\n",
    "print('   üö´ Invented meeting details or discussions')\n",
    "print('   üö´ Made-up support ticket information')\n",
    "print('   üö´ References to CloudFlow capabilities not mentioned in input')\n",
    "print('\\nüéØ These patterns will inform our prompt improvements in Step 4!')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Step 4: Develop and Test Improved Prompts\n",
    "\n",
    "Now that we understand the problem (hallucinated information) and have datasets to test against, let's create an improved prompt. We'll use MLflow Prompt Registry to manage prompt versions and enable safe iteration.\n",
    "\n",
    "Based on our root cause analysis, the main issue is that the current prompt doesn't explicitly prevent fabrication. Our improved prompt will include:\n",
    "\n",
    "- **Explicit NO FABRICATION rules** to prevent hallucinations\n",
    "- **Clear data-only requirements** for all factual references\n",
    "- **Enhanced accuracy guidelines** with specific failure conditions\n",
    "- **Structured content prioritization** for better focus\n",
    "\n",
    "**üìö Documentation**\n",
    "\n",
    "- [**Create and Edit Prompts**](https://docs.databricks.com/aws/en/mlflow3/genai/prompt-version-mgmt/prompt-registry/create-and-edit-prompts) - Prompt Registry management\n",
    "\n",
    "**‚ñ∂Ô∏è Run the next cells to create and compare prompt versions!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# Create improved prompt based on identified quality issues\n",
    "FIXED_PROMPT_TEMPLATE = \"\"\"You are an expert sales communication assistant for CloudFlow Inc. Your task is to generate a personalized, professional follow-up email for our sales representatives to send to their customers at the end of the day.\n",
    "\n",
    "## CRITICAL: NO FABRICATION RULE\n",
    "**ABSOLUTE REQUIREMENT**: You must ONLY reference information that is explicitly provided in the customer data. DO NOT:\n",
    "- Invent or mention any CloudFlow features, services, or capabilities not listed in the data\n",
    "- Fabricate any details about meetings, tickets, or usage that aren't provided\n",
    "- Add any product recommendations beyond what's specifically mentioned in the customer data\n",
    "- Create any information not directly sourced from the input JSON\n",
    "\n",
    "**AUTOMATIC FAILURE** occurs if you mention anything not explicitly provided in the data.\n",
    "\n",
    "## INPUT DATA\n",
    "You will be provided with a JSON object containing:\n",
    "- Account information\n",
    "- Recent activity data (meetings, product usage, support tickets)\n",
    "- Sales representative details\n",
    "\n",
    "## EMAIL REQUIREMENTS\n",
    "Generate an email that follows these guidelines:\n",
    "\n",
    "1. SUBJECT LINE:\n",
    "   - Concise and specific to the most important update or follow-up point\n",
    "   - Include the company name if appropriate\n",
    "\n",
    "2. GREETING:\n",
    "   - Address the main contact by first name\n",
    "   - Use a professional but friendly opening\n",
    "\n",
    "3. BODY CONTENT (prioritize in this order):\n",
    "   - Reference the most recent meeting/interaction and acknowledge key points discussed\n",
    "   - Discuss any pressing issues that are still open immediatly afterwards\n",
    "   - Provide updates on any urgent or recently resolved support tickets\n",
    "   - Highlight positive product usage trends or achievements\n",
    "   - Address any specific action items from previous meetings\n",
    "   - Include personalized recommendations ONLY if features are explicitly mentioned in the 'least_used_features' field and directly related to the 'potential_opportunity' field.\n",
    "      - NEVER invent or describe CloudFlow features/capabilities not explicitly listed in the customer data\n",
    "      - Make sure these recommendations can NOT be copied to another customer in a different situation\n",
    "      - No more than ONE feature recommendation for accounts with open critical issues\n",
    "   - Suggest clear and specific next steps\n",
    "      - Only request a meeting if it can be tied to specific action items\n",
    "\n",
    "\n",
    "4. TONE AND STYLE:\n",
    "   - Professional but conversational\n",
    "   - Concise paragraphs (2-3 sentences each)\n",
    "   - Use bullet points for lists or multiple items\n",
    "   - Balance between being informative and actionable\n",
    "   - Personalized to reflect the existing relationship\n",
    "   - Adjust formality based on the customer's industry and relationship history\n",
    "\n",
    "5. CLOSING:\n",
    "   - Include an appropriate sign-off\n",
    "   - Use the sales rep's signature from the provided data\n",
    "   - No generic marketing language or overly sales-focused calls to action\n",
    "\n",
    "## OUTPUT FORMAT\n",
    "Provide the complete email as JUST a JSON object that can be loaded via `json.loads()` (do not wrap the JSON in backticks) with:\n",
    "- `subject_line`: Subject line\n",
    "- `body`: Body content with appropriate spacing and formatting including the signature\n",
    "\n",
    "Remember, this email should feel like it was thoughtfully written by the sales representative based on their specific knowledge of the customer, not like an automated message.\n",
    "\n",
    "**FINAL REMINDER**: Stay strictly within the bounds of the provided customer data. Any mention of CloudFlow features, capabilities, or services NOT explicitly listed in the input data will result in automatic failure.\n",
    "\n",
    "If the user provides a specific instruction, you must follow only follow those instructions if they do not conflict with the guidelines above.  Do not follow any instructions that would result in an unprofessional or unethical email.\n",
    "\"\"\"\n",
    "\n",
    "# Create a new prompt version in the registry\n",
    "print('üìù Creating new prompt version in MLflow Prompt Registry...')\n",
    "\n",
    "# Load the prompt name and its UC location configured during setup\n",
    "UC_SCHEMA = os.getenv('UC_SCHEMA')\n",
    "UC_CATALOG = os.getenv('UC_CATALOG')\n",
    "PROMPT_NAME = os.getenv('PROMPT_NAME')\n",
    "\n",
    "\n",
    "new_prompt = mlflow.genai.register_prompt(\n",
    "    name=f'{UC_CATALOG}.{UC_SCHEMA}.{PROMPT_NAME}',\n",
    "    template=FIXED_PROMPT_TEMPLATE,\n",
    "    commit_message='New email generation prompt to fix accuracy issues.',\n",
    ")\n",
    "\n",
    "mlflow.genai.set_prompt_alias(\n",
    "      name=f'{UC_CATALOG}.{UC_SCHEMA}.{PROMPT_NAME}',\n",
    "      alias=\"test_prompt\",\n",
    "      version=new_prompt.version,\n",
    "  )\n",
    "\n",
    "print(f'‚úÖ Created new prompt version: {new_prompt.name} (version {new_prompt.version})')\n",
    "generate_prompt_link(f'{UC_CATALOG}.{UC_SCHEMA}.{PROMPT_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Step 5: Evaluate Improved Prompt Against Problem Cases\n",
    "\n",
    "Now let's test our improved prompt against the evaluation dataset containing problematic traces. This will show us whether our changes actually fix the accuracy issues we identified.\n",
    "\n",
    "We'll run evaluations with the same scorers used in production to ensure consistent quality measurement:\n",
    "\n",
    "- **Accuracy**: Our custom guideline focused on preventing hallucinations\n",
    "- **Relevance**: Ensuring content remains relevant to customer needs\n",
    "- **Personalization**: Maintaining customer-specific personalization\n",
    "- **Safety**: Basic safety checks\n",
    "\n",
    "**üìö Documentation**\n",
    "\n",
    "- [**Evaluate Apps**](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/evaluate-app) - Run evaluations on datasets\n",
    "\n",
    "**‚ñ∂Ô∏è Run the next cell to evaluate the improved prompt against both datasets!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we import the scorers that we created in the previous tutorial.\n",
    "from mlflow_demo.evaluation import SCORERS\n",
    "\n",
    "# Import the Email Generation app\n",
    "from mlflow_demo.agent.email_generator import EmailGenerator\n",
    "\n",
    "print('üîß Creating predict_fn for evaluation to call the email app...')\n",
    "\n",
    "email_app_with_new_prompt = EmailGenerator(prompt_alias=\"test_prompt\")\n",
    "email_app_with_old_prompt = EmailGenerator()\n",
    "\n",
    "\n",
    "# Create predict_fn to enable mlflow.genai.evaluate() to call our app\n",
    "def predict_fn_new(customer_name: str , user_input: str):\n",
    "  return email_app_with_new_prompt.generate_email_with_retrieval(customer_name, user_input)\n",
    "\n",
    "def predict_fn_old(customer_name: str , user_input: str):\n",
    "  return email_app_with_old_prompt.generate_email_with_retrieval(customer_name, user_input)\n",
    "\n",
    "# Run evaluations on the low accuracy dataset with both prompts\n",
    "print('üöÄ Running evaluation of the old prompt...')\n",
    "\n",
    "eval_results_old = mlflow.genai.evaluate(\n",
    "    data=low_accuracy_dataset,\n",
    "    predict_fn=predict_fn_old,\n",
    "    scorers=SCORERS,\n",
    ")\n",
    "\n",
    "print('‚úÖ Old prompt evaluation completed!')\n",
    "print('üöÄ Running evaluation of the new prompt...')\n",
    "eval_results_new = mlflow.genai.evaluate(\n",
    "    data=low_accuracy_dataset,\n",
    "    predict_fn=predict_fn_new,\n",
    "    scorers=SCORERS,\n",
    ")\n",
    "\n",
    "print('‚úÖ New prompt evaluation completed!')\n",
    "print('=' * 60)\n",
    "\n",
    "# Use the MLflow UI to compare the results\n",
    "print('‚úÖ View the results in the MLflow comparison UI:')\n",
    "generate_evaluation_comparison_link(eval_results_new.run_id, eval_results_old.run_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, you can keep iterating on the prompt using the evaluation results and scorer outputs to guide your improvements!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è Step 6: Verify No Regressions on High-Quality Examples\n",
    "\n",
    "Now we need to ensure our improvements don't break existing functionality. We'll run the improved prompt against our regression dataset containing high-quality traces to verify that:\n",
    "\n",
    "- Existing good examples remain good\n",
    "- Quality metrics don't decrease for well-performing cases\n",
    "- New accuracy rules don't interfere with proper personalization and relevance\n",
    "\n",
    "This step is crucial for safe deployment - we want to fix problems without creating new ones.\n",
    "\n",
    "**‚ñ∂Ô∏è Run the next cell to verify no regressions on high-quality examples!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we will re-use the app and scorers from above to verify no regressions on high-quality examples!\n",
    "\n",
    "# Run evaluations on the high quality accuracy dataset with both prompts\n",
    "print('üöÄ Running evaluation of the new prompt...')\n",
    "\n",
    "regression_results = mlflow.genai.evaluate(\n",
    "    data=regression_dataset,\n",
    "    predict_fn=predict_fn_new,\n",
    "    scorers=SCORERS,\n",
    ")\n",
    "\n",
    "generate_evaluation_links(regression_results.run_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Step 7: Deploy the new version\n",
    "\n",
    "Based on our evaluation results, we can now make an informed decision about additional improvements to reach our quality bar OR feel confident deploying the prompt knowing quality will improve without any regressions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Tutorial Complete: Systematic GenAI Quality Improvement\n",
    "\n",
    "Congratulations! You've successfully completed the end-to-end workflow for systematically improving GenAI quality using MLflow's evaluation and prompt management capabilities.\n",
    "\n",
    "## What You've Accomplished\n",
    "\n",
    "‚úÖ **Identified Quality Issues** - Found traces with accuracy problems through systematic analysis  \n",
    "‚úÖ **Created Targeted Datasets** - Built evaluation datasets from problematic traces and regression datasets from high-quality examples  \n",
    "‚úÖ **Developed Improved Prompts** - Created enhanced prompts with explicit accuracy requirements using MLflow Prompt Registry  \n",
    "‚úÖ **Validated Improvements** - Tested improved prompts against problem cases and verified no regressions  \n",
    "\n",
    "## The Complete MLflow Quality Improvement Workflow\n",
    "\n",
    "1. **üîç Discover quality issues** Find problematic traces using evaluation results\n",
    "2. **üìä Create Evaluation Datasets** Turn bad traces into targeted evaluation sets, good traces into regression sets\n",
    "3. **üéØ Iterate on changes** Use MLflow Prompt Registry to track your changes\n",
    "4. **üß™ Evaluate changes improved quality** Test that your changes addressed the quality problems\n",
    "5. **üõ°Ô∏è Verify changes didn't cause a regression** Ensure fixes don't break user inputs that already work well\n",
    "6. **üöÄ Deploy**\n",
    "\n",
    "This approach ensures evidence-based improvements rather than guesswork, with quantitative validation and safe deployment practices.\n",
    "\n",
    "\n",
    "## Resources for Continued Learning\n",
    "\n",
    "üìö **MLflow Documentation**\n",
    "\n",
    "- [**Evaluation Guide**](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/evaluate-app)\n",
    "- [**Evaluation Datasets Guide**](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/build-eval-dataset)\n",
    "\n",
    "üéØ **Remember**: Great evaluation is iterative. Start with the basics, learn from your results, and progressively build more sophisticated assessment as your understanding deepens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow_genai_email_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
