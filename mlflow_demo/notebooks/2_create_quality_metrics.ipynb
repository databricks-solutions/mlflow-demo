{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate quality using LLM and code-based scorers\n",
    "\n",
    "## Scale your human expertise with LLM scorers for automated quality evaluation that is tuned to your use case\n",
    "\n",
    "LLM scorers are AI-powered quality assessment tools that scale human expertise to evaluate GenAI quality automatically - in development and production. They assess semantic correctness, style, safety, and relative quality - answering questions like \"Does this answer correctly?\" and \"Is this appropriate for our brand?\"\n",
    "\n",
    "MLflow has 2 flavors of judges:\n",
    "\n",
    "- **Built-in Judges** - Research-backed judges for safety, hallucination, retrieval quality, and relevance\n",
    "- **Custom Judges** - Tune our research-backed LLM judges to your business needs and human expert judgment\n",
    "\n",
    "MLflow also supports _custom code-based metrics_, so if the built-in judges don't fit your use case, you can write your own.\n",
    "\n",
    "The same judges can be used to both evaluate quality in development and monitor quality in production.\n",
    "\n",
    "![demo-eval](https://i.imgur.com/M3kLBHF.gif)\n",
    "\n",
    "This notebook demonstrates how to create and use LLM judges to evaluate GenAI quality. You'll learn to use both built-in judges and create custom guidelines that align with your business requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages (only required if running in a Databricks Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -r ../../requirements.txt\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Load environment variables and verify MLflow configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T13:46:55.383312Z",
     "iopub.status.busy": "2025-07-29T13:46:55.383037Z",
     "iopub.status.idle": "2025-07-29T13:46:55.398592Z",
     "shell.execute_reply": "2025-07-29T13:46:55.398242Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import mlflow\n",
    "from mlflow_demo.utils import *\n",
    "\n",
    "if mlflow.utils.databricks_utils.is_in_databricks_notebook():\n",
    "  print(\"Running in Databricks Notebook\")\n",
    "  setup_databricks_notebook_env()\n",
    "else:\n",
    "  print(\"Running in Local IDE\")\n",
    "  setup_local_ide_env()\n",
    "\n",
    "# Verify key variables are loaded\n",
    "print('=== Environment Setup ===')\n",
    "print(f'DATABRICKS_HOST: {os.getenv(\"DATABRICKS_HOST\")}')\n",
    "print(f'MLFLOW_EXPERIMENT_ID: {os.getenv(\"MLFLOW_EXPERIMENT_ID\")}')\n",
    "print(f'LLM_MODEL: {os.getenv(\"LLM_MODEL\")}')\n",
    "print(f'UC_CATALOG: {os.getenv(\"UC_CATALOG\")}')\n",
    "print(f'UC_SCHEMA: {os.getenv(\"UC_SCHEMA\")}')\n",
    "print('‚úÖ Environment variables loaded successfully!')\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T13:46:55.427002Z",
     "iopub.status.busy": "2025-07-29T13:46:55.426872Z",
     "iopub.status.idle": "2025-07-29T13:46:56.923535Z",
     "shell.execute_reply": "2025-07-29T13:46:56.923282Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import demo utilities\n",
    "from mlflow_demo.utils.mlflow_helpers import get_mlflow_experiment_id, generate_evaluation_links\n",
    "\n",
    "print('‚úÖ All imports successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Step 1: Understanding MLflow Evaluation\n",
    "\n",
    "A **scorer** is what looks at a trace and performs evaluation on that trace, then returns feedback which gets attached to the trace. This is the core building block of MLflow's evaluation system.\n",
    "\n",
    "### üéØ **What's Inside a Scorer?**\n",
    "\n",
    "Within a scorer, you can use:\n",
    "\n",
    "- **LM judges** - Feed aspects of the trace to an LLM to perform assessments (e.g., \"Is this response safe?\")\n",
    "- **Deterministic code** - Count tokens, measure latency, check formatting, validate compliance\n",
    "- **Combination of both** - Use LLM reasoning for content quality + code for objective measures\n",
    "\n",
    "### üöÄ **The Complete Workflow: Scorer ‚Üí Evaluate ‚Üí Monitor**\n",
    "\n",
    "**1. Create Scorers** - Build evaluation logic for your quality dimensions\n",
    "\n",
    "```python\n",
    "scorers = [Safety(), RelevanceToQuery(), your_custom_scorer]\n",
    "```\n",
    "\n",
    "**2. Run offline evaluation with `mlflow.genai.evaluate()`** - MLflow handles the coordination\n",
    "\n",
    "```python\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=your_traces,        # Your traces or test data\n",
    "    scorers=scorers          # List of scorers to run\n",
    ")\n",
    "```\n",
    "\n",
    "MLflow's `evaluate()` function automatically:\n",
    "\n",
    "- Feeds your traces through all scorers in parallel\n",
    "- Aggregates results into structured metrics\n",
    "- Stores the results in your MLflow Experiment\n",
    "\n",
    "**3. Use for Online Monitoring** - Use the same scorers to monitor production quality - we cover these steps in notebook 5.\n",
    "\n",
    "**üîç Behind the Scenes**: For each trace, each scorer extracts the data it needs, runs its evaluation logic (LM judges, code, or both), and returns structured feedback that MLflow organizes into actionable insights.\n",
    "\n",
    "**‚ñ∂Ô∏è Run the next cells to see different types of scorers in action**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Step 2: Predefined LLM Judge Scorers\n",
    "\n",
    "MLflow provides research-backed judges for common evaluation needs. Here, we will use a few of the built-in judge scorers that apply to our email generation use case.\n",
    "\n",
    "Here, we will use three predefined scorers to provide a basic quality assessment of our email generation app:\n",
    "\n",
    "üéØ `RelevanceToQuery`: Does the generated email directly address the user's request?\n",
    "\n",
    "- Checks if the email content stays focused on what the user asked for\n",
    "- Critical for maintaining professional relevance in business communications\n",
    "\n",
    "üè† `RetrievalGroundedness`: Is the email content grounded in the retrieved customer data?\n",
    "\n",
    "- Checks for hallucination of customer details, meeting notes, or account information in the generated email\n",
    "- Essential for maintaining trust and accuracy in customer communications\n",
    "\n",
    "üîí `Safety`: Does the generated email avoid harmful or inappropriate content?\n",
    "\n",
    "- Catches potential toxic content that could damage business relationships\n",
    "- Critical safeguard for any customer-facing communications\n",
    "\n",
    "**üìö Documentation**\n",
    "\n",
    "- [**Predefined Judge Scorers**](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/predefined-judge-scorers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import RelevanceToQuery, RetrievalGroundedness, Safety\n",
    "import mlflow\n",
    "\n",
    "# A Scorer operates on a MLflow Trace, so let's retrieve a few Traces:\n",
    "print('\\nüîç Loading recent traces from our email demo app...')\n",
    "\n",
    "# Load recent traces for evaluation\n",
    "traces = mlflow.search_traces(\n",
    "    max_results=3,\n",
    "    filter_string='status = \"OK\"',\n",
    "    order_by=['timestamp DESC'],\n",
    ")\n",
    "print(f\"‚úÖ Found {len(traces)} traces for evaluation\")\n",
    "\n",
    "# Now, let's run evaluation using these scorers\n",
    "\n",
    "eval_results = mlflow.genai.evaluate(data=traces, scorers=[RelevanceToQuery(), RetrievalGroundedness(), Safety()])\n",
    "\n",
    "print(f\"\\nüìä Evaluation completed!\")\n",
    "print(f\"üÜî Run ID: {eval_results._run_id}\")\n",
    "\n",
    "# Generate and display evaluation links\n",
    "generate_evaluation_links(eval_results._run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Step 2A: Controlling Data Fields for Built-in Scorers\n",
    "\n",
    "## The Challenge: One Size Doesn't Fit All\n",
    "\n",
    "While predefined scorers like `RelevanceToQuery`, `RetrievalGroundedness`, and `Safety` are powerful, they use **default data extraction** that may not always match your specific needs.\n",
    "\n",
    "### Common Issue: Email Subject Line Groundedness\n",
    "\n",
    "In our email generation app, we've noticed that `RetrievalGroundedness` sometimes flags email **subject lines** as \"not grounded\" in the retrieved customer data. However, this is often acceptable because:\n",
    "\n",
    "- üìß **Subject lines are often creative summaries** - \"Follow-up on our conversation\"\n",
    "- üéØ **They don't need to reference specific data points** - Unlike the email body\n",
    "- ‚úÖ **They can be professionally generic** - \"Checking in\" or \"Next steps\"\n",
    "\n",
    "### The Solution: Custom Data Extraction + Predefined Judges\n",
    "\n",
    "Instead of writing entirely new evaluation logic, we can:\n",
    "\n",
    "1. **Wrap the proven `is_grounded` judge** - Keep the research-backed evaluation logic\n",
    "2. **Customize data extraction** - Pass only the email body (not subject) to the judge\n",
    "3. **Maintain evaluation quality** - Get accurate groundedness assessment for content that matters\n",
    "\n",
    "This is the **hybrid approach** - combining the best of predefined judges with custom control over data extraction.\n",
    "\n",
    "**üìö Reference**:\n",
    "\n",
    "- [**Custom Scorers Documentation**](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/custom-scorers#example-2-wrap-a-predefined-llm-judge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Example: Custom Email Body Groundedness Scorer\n",
    "\n",
    "from mlflow.genai.judges import is_grounded\n",
    "from mlflow.genai.scorers import scorer\n",
    "import re\n",
    "\n",
    "@scorer\n",
    "def email_is_grounded(trace):\n",
    "    \"\"\"\n",
    "    Custom groundedness scorer that evaluates only the email body content,\n",
    "    excluding the subject line to avoid false negatives on creative/generic subjects.\n",
    "\n",
    "    This demonstrates how to wrap the proven is_grounded judge with custom data extraction.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from mlflow.genai.judges import is_grounded\n",
    "\n",
    "    # Extract the original request\n",
    "    outputs = json.loads(trace.data.response)\n",
    "    email_body = outputs.get('email_body')\n",
    "    user_input = outputs.get('user_input')\n",
    "    input_facts = trace.search_spans(span_type=\"RETRIEVER\")[0].outputs\n",
    "\n",
    "    if user_input is None or len(user_input) == 0:\n",
    "      request = \"Generate an email based on the provided context.\"\n",
    "    else:\n",
    "      request = \"Generate an email based on the provided context, considering the user's request: \" + user_input\n",
    "\n",
    "    # Use the proven is_grounded judge with our extracted email body\n",
    "    return is_grounded(request=request, response=email_body, context=input_facts)\n",
    "\n",
    "# A Scorer operates on a MLflow Trace, so let's retrieve a few Traces:\n",
    "print('\\nüîç Loading recent traces from our email demo app...')\n",
    "\n",
    "# Load recent traces for evaluation\n",
    "traces = mlflow.search_traces(\n",
    "    max_results=3,\n",
    "    filter_string='status = \"OK\"',\n",
    "    order_by=['timestamp DESC'],\n",
    ")\n",
    "print(f\"‚úÖ Found {len(traces)} traces for evaluation\")\n",
    "\n",
    "# Now, let's run evaluation using this scorer\n",
    "\n",
    "eval_results = mlflow.genai.evaluate(data=traces, scorers=[email_is_grounded])\n",
    "\n",
    "print(f\"\\nüìä Evaluation completed!\")\n",
    "print(f\"üÜî Run ID: {eval_results._run_id}\")\n",
    "\n",
    "# Generate and display evaluation links\n",
    "generate_evaluation_links(eval_results._run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Step 3: Custom Guidelines using Native Judge Classes\n",
    "\n",
    "While built-in judges handle common quality aspects, your business has specific requirements. We'll create custom guidelines using native MLflow judge classes for direct instantiation.\n",
    "\n",
    "## What are Guidelines?\n",
    "\n",
    "**Guidelines** are a simple way to codify your business-specific rules as **natural language criteria** that result in **pass/fail** evaluation. Guidelines can be a short sentence or a longer set of criteria.\n",
    "\n",
    "Like before, we can use the predefined Guidelines scorer directly or wrap it in a custom scorer to control data extraction and processing. This parallels the pattern shown in the previous cells where you had predefined scorers (Step 2) and then hybrid approaches with custom data extraction (Step 2A).\n",
    "\n",
    "### Why Guidelines?\n",
    "\n",
    "- ‚úÖ **Easy to explain** to business stakeholders\n",
    "- ‚úÖ **Domain experts can write them** directly - no coding required\n",
    "- ‚úÖ **Clear pass/fail decisions** - perfect for compliance and business rules\n",
    "- ‚úÖ **Starting point** - recommended before complex prompt-based judges\n",
    "\n",
    "### How Guidelines Work\n",
    "\n",
    "1. **You write the rule** in plain language: _\"The email must reference specific customer data\"_\n",
    "2. **An LLM evaluates** whether the content passes or fails the guideline\n",
    "3. **You get clear feedback** with rationale for the decision\n",
    "\n",
    "## Email Generation Guidelines\n",
    "\n",
    "For our email app, we'll create simple guidelines to ensure quality:\n",
    "\n",
    "1. **Tone of Voice** - The tone must be professional\n",
    "2. **Accuracy** - All facts must come from provided data\n",
    "3. **Personalization** - Emails must be tailored to specific customers\n",
    "4. **Relevance** - Content must be prioritized by urgency\n",
    "\n",
    "**üìö Reference**:\n",
    "\n",
    "- [**Guidelines Scorers Documentation**](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/custom-judge/meets-guidelines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T13:47:04.119214Z",
     "iopub.status.busy": "2025-07-29T13:47:04.118716Z",
     "iopub.status.idle": "2025-07-29T13:47:04.126436Z",
     "shell.execute_reply": "2025-07-29T13:47:04.125955Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import Guidelines\n",
    "from mlflow.genai.judges import meets_guidelines\n",
    "from mlflow.genai.scorers import scorer\n",
    "import mlflow\n",
    "\n",
    "# Tone of voice Guideline - Ensure professional tone\n",
    "tone = Guidelines(\n",
    "  name='tone',\n",
    "  guidelines=\"\"\"The response maintains a professional tone.\"\"\")\n",
    "\n",
    "# Accuracy Guideline - Verify all facts come from provided data\n",
    "@scorer\n",
    "def accuracy(trace):\n",
    "    \"\"\"\n",
    "    Custom accuracy scorer that evaluates only the email body content,\n",
    "    excluding the subject line to avoid false negatives on creative/generic subjects.\n",
    "\n",
    "    This demonstrates how to wrap the proven Guidelines judge with custom data extraction.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from mlflow.genai.judges import meets_guidelines\n",
    "    # Extract the original request\n",
    "    outputs = json.loads(trace.data.response)\n",
    "    email_body = outputs.get('email_body')\n",
    "    user_input = outputs.get('user_input')\n",
    "    input_facts = trace.search_spans(span_type=\"RETRIEVER\")[0].outputs\n",
    "\n",
    "    accuracy_guideline = \"\"\"The email_body correctly references all factual information from the provided_info based on these rules:\n",
    "- All factual information must be directly sourced from the provided data with NO fabrication\n",
    "- Names, dates, numbers, and company details must be 100% accurate with no errors\n",
    "- Meeting discussions must be summarized with the exact same sentiment and priority as presented in the data\n",
    "- Support ticket information must include correct ticket IDs, status, and resolution details when available\n",
    "- All product usage statistics must be presented with the same metrics provided in the data\n",
    "- No references to CloudFlow features, services, or offerings unless specifically mentioned in the customer data\n",
    "- AUTOMATIC FAIL if any information is mentioned that is not explicitly provided in the data\n",
    "- It is OK if the email_body follows the user_input request to omit certain facts, as long as no fabricated facts are introduced.\"\"\"\n",
    "\n",
    "    # Use the proven Guidelines judge with our extracted email body\n",
    "    return meets_guidelines(guidelines=accuracy_guideline, context={'provided_info': input_facts, 'email': email_body, 'user_input': user_input})\n",
    "# Personalization Guideline - Ensure emails are tailored to specific customers\n",
    "@scorer\n",
    "def personalized(trace):\n",
    "    \"\"\"\n",
    "    Custom personalization scorer that evaluates only the email body content,\n",
    "    excluding the subject line to avoid false negatives on creative/generic subjects.\n",
    "\n",
    "    This demonstrates how to wrap the proven Guidelines judge with custom data extraction.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from mlflow.genai.judges import meets_guidelines\n",
    "    # Extract the original request\n",
    "    outputs = json.loads(trace.data.response)\n",
    "    email_body = outputs.get('email_body')\n",
    "    user_input = outputs.get('user_input')\n",
    "    input_facts = trace.search_spans(span_type=\"RETRIEVER\")[0].outputs\n",
    "\n",
    "    personalized_guideline = \"\"\"The email_body demonstrates clear personalization based on the provided_info based on these rules:\n",
    "- Email must begin by referencing the most recent meeting/interaction\n",
    "- Immediately next, the email must address the customer's MOST pressing concern as evidenced in the data\n",
    "- Content structure must be customized based on the account's health status (critical issues first for \"Fair\" or \"Poor\" accounts)\n",
    "- Industry-specific language must be used that reflects the customer's sector\n",
    "- Recommendations must ONLY reference features that are:\n",
    "  a) Listed as \"least_used_features\" in the data, AND\n",
    "  b) Directly related to the \"potential_opportunity\" field\n",
    "- Relationship history must be acknowledged (new vs. mature relationship)\n",
    "- Deal stage must influence communication approach (implementation vs. renewal vs. growth)\n",
    "- AUTOMATIC FAIL if recommendations could be copied to another customer in a different situation\"\"\"\n",
    "\n",
    "    # Use the proven Guidelines judge with our extracted email body\n",
    "    return meets_guidelines(guidelines=personalized_guideline, context={'provided_info': input_facts, 'email': email_body, 'user_input': user_input})\n",
    "\n",
    "# Relevance Guideline - Prioritize content by urgency\n",
    "@scorer\n",
    "def relevance(trace):\n",
    "    \"\"\"\n",
    "    Custom relevance scorer that evaluates only the email body content,\n",
    "    excluding the subject line to avoid false negatives on creative/generic subjects.\n",
    "\n",
    "    This demonstrates how to wrap the proven Guidelines judge with custom data extraction.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from mlflow.genai.judges import meets_guidelines\n",
    "    # Extract the original request\n",
    "    outputs = json.loads(trace.data.response)\n",
    "    email_body = outputs.get('email_body')\n",
    "    user_input = outputs.get('user_input')\n",
    "    input_facts = trace.search_spans(span_type=\"RETRIEVER\")[0].outputs\n",
    "\n",
    "    relevance_guideline = \"\"\"The email_body prioritizes content that matters to the recipient in the provided_info based on these rules:\n",
    "- Critical support tickets (status=\"Open (Critical)\") must be addressed after the greeting, reference to the most recent interaction, any pleasantries, and references to closed tickets\n",
    "- Time-sensitive action items must be addressed before general updates\n",
    "- Content must be ordered by descending urgency as defined by:\n",
    "  1. Critical support issues\n",
    "  2. Action items explicitly stated in most recent meeting\n",
    "  3. Upcoming renewal if within 30 days\n",
    "  4. Recently resolved issues\n",
    "  5. Usage trends and recommendations\n",
    "- No more than ONE feature recommendation for accounts with open critical issues\n",
    "- No mentions of company news, product releases, or success stories not directly requested by the customer\n",
    "- No calls to action unrelated to the immediate needs in the data\n",
    "- AUTOMATIC FAIL if the email requests a meeting without being tied to a specific action item or opportunity in the data\"\"\"\n",
    "\n",
    "    # Use the proven Guidelines judge with our extracted email body\n",
    "    return meets_guidelines(guidelines=relevance_guideline, context={'provided_info': input_facts, 'email': email_body, 'user_input': user_input})\n",
    "\n",
    "\n",
    "# A Scorer operates on a MLflow Trace, so let's retrieve a few Traces:\n",
    "print('\\nüîç Loading recent traces from our email demo app...')\n",
    "\n",
    "# Load recent traces for evaluation\n",
    "traces = mlflow.search_traces(\n",
    "    max_results=3,\n",
    "    filter_string='status = \"OK\"',\n",
    "    order_by=['timestamp DESC'],\n",
    ")\n",
    "print(f\"‚úÖ Found {len(traces)} traces for evaluation\")\n",
    "\n",
    "# Now, let's run evaluation using this scorer\n",
    "\n",
    "eval_results = mlflow.genai.evaluate(data=traces, scorers=[tone, accuracy, personalized, relevance])\n",
    "\n",
    "print(f\"\\nüìä Evaluation completed!\")\n",
    "print(f\"üÜî Run ID: {eval_results._run_id}\")\n",
    "\n",
    "# Generate and display evaluation links\n",
    "generate_evaluation_links(eval_results._run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Step 5: Custom Prompt-based Judges (Advanced)\n",
    "\n",
    "For complex, nuanced evaluations that require sophisticated reasoning, you can create custom prompt-based judges with full control over:\n",
    "\n",
    "1. **Judge prompts** - Custom evaluation instructions\n",
    "2. **Output formats** - Multiple value types (scores, categories, detailed feedback)\n",
    "3. **Complex logic** - Multi-dimensional assessment\n",
    "\n",
    "This approach is best when you have complex, nuanced evaluations where you need full control over the scorer's prompt or need to have the scorer specify multiple output values, for example, \"great\", \"ok\", \"bad\".\n",
    "\n",
    "Here, we will create a custom prompt scorer that evaluates whether an email will drive positive business outcomes by assessing customer satisfaction, relationship impact, revenue opportunities, issue resolution, and trust-building potential.\n",
    "\n",
    "**üìö Reference**:\n",
    "\n",
    "- [**Prompt-based Scorers Documentation**](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/custom-judge/create-prompt-judge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Custom Prompt-based Judges - Advanced Control\n",
    "\n",
    "from mlflow.genai.scorers import scorer\n",
    "from mlflow.genai.judges import custom_prompt_judge\n",
    "\n",
    "\n",
    "# Business Impact Assessment Judge\n",
    "\n",
    "@scorer\n",
    "def business_impact(trace):\n",
    "    \"\"\"\n",
    "    Assess whether an email response will drive positive business outcomes\n",
    "    \"\"\"\n",
    "\n",
    "    import json\n",
    "\n",
    "    business_impact_prompt = \"\"\"\n",
    "You are a business value analyst evaluating whether an email response will drive positive business outcomes.\n",
    "\n",
    "EMAIL RESPONSE: {{email_body}}\n",
    "\n",
    "USER'S INSTRUCTION: {{user_input}}\n",
    "\n",
    "CUSTOMER DATA: {{input_facts}}\n",
    "\n",
    "BUSINESS IMPACT EVALUATION:\n",
    "Assess whether this email will likely result in:\n",
    "1. Customer satisfaction improvement\n",
    "2. Relationship strengthening\n",
    "3. Revenue protection/growth opportunities\n",
    "4. Issue resolution acceleration\n",
    "5. Trust building\n",
    "\n",
    "Consider:\n",
    "- Does the email address the customer's most pressing needs?\n",
    "- Will it move the relationship forward positively?\n",
    "- Does it demonstrate value and expertise?\n",
    "- Is it likely to generate a positive response?\n",
    "\n",
    "You must choose one of the following categories:\n",
    "\n",
    "[[high]]: Email likely to drive significant positive business impact across multiple dimensions\n",
    "[[medium]]: Email likely to drive some positive business impact in key areas\n",
    "[[low]]: Email unlikely to drive meaningful business impact\n",
    "[[negative]]: Email could harm business relationship or customer satisfaction\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    # Extract the original request\n",
    "    outputs = json.loads(trace.data.response)\n",
    "    email_body = outputs.get('email_body')\n",
    "    user_input = outputs.get('user_input')\n",
    "    input_facts = trace.search_spans(span_type=\"RETRIEVER\")[0].outputs\n",
    "\n",
    "    # Create business impact judge\n",
    "    impact_judge = custom_prompt_judge(\n",
    "        name=\"business_impact_assessment\",\n",
    "        prompt_template=business_impact_prompt,\n",
    "        numeric_values={\n",
    "            \"high\": 4,\n",
    "            \"medium\": 3,\n",
    "            \"low\": 2,\n",
    "            \"negative\": 1\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return impact_judge(email_body=email_body, user_input=user_input, input_facts=input_facts)\n",
    "\n",
    "# A Scorer operates on a MLflow Trace, so let's retrieve a few Traces:\n",
    "print('\\nüîç Loading recent traces from our email demo app...')\n",
    "\n",
    "# Load recent traces for evaluation\n",
    "traces = mlflow.search_traces(\n",
    "    max_results=3,\n",
    "    filter_string='status = \"OK\"',\n",
    "    order_by=['timestamp DESC'],\n",
    ")\n",
    "print(f\"‚úÖ Found {len(traces)} traces for evaluation\")\n",
    "\n",
    "# Now, let's run evaluation using this scorer\n",
    "\n",
    "eval_results = mlflow.genai.evaluate(data=traces, scorers=[business_impact])\n",
    "\n",
    "print(f\"\\nüìä Evaluation completed!\")\n",
    "print(f\"üÜî Run ID: {eval_results._run_id}\")\n",
    "\n",
    "# Generate and display evaluation links\n",
    "generate_evaluation_links(eval_results._run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíª Step 5: Code-based Scorers (Pure Python Evaluation)\n",
    "\n",
    "For deterministic, measurable criteria that don't require LLM reasoning, code-based scorers provide:\n",
    "\n",
    "1. **Fast execution** - No LLM API calls\n",
    "2. **Deterministic results** - Same input always produces same output\n",
    "3. **Cost-effective** - No token usage costs\n",
    "4. **Precise control** - Exact logic for measurable criteria\n",
    "\n",
    "Perfect for checking compliance, formatting, length requirements, and other objective criteria.\n",
    "\n",
    "**üìö Reference**:\n",
    "\n",
    "- [**Code-based Scorers Documentation**](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/custom-scorers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import scorer\n",
    "import mlflow\n",
    "from mlflow.entities.assessment import Feedback\n",
    "\n",
    "@scorer\n",
    "def email_format_compliance(trace):\n",
    "    \"\"\"\n",
    "    Code-based scorer that checks email formatting, structure, and professional standards\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import re\n",
    "\n",
    "    # Extract the email content\n",
    "    outputs = json.loads(trace.data.response)\n",
    "    email_body = outputs.get('email_body', '')\n",
    "    email_subject = outputs.get('email_subject', '')\n",
    "\n",
    "    score = 0\n",
    "    issues = []\n",
    "    max_score = 5\n",
    "\n",
    "    # 1. Check subject line exists and is reasonable length (1 point)\n",
    "    if email_subject and 10 <= len(email_subject) <= 80:\n",
    "        score += 1\n",
    "    else:\n",
    "        issues.append(\"Subject line missing or improper length\")\n",
    "\n",
    "    # 2. Check email has proper greeting (1 point)\n",
    "    greeting_patterns = [r'\\bhi\\b', r'\\bhello\\b', r'\\bdear\\b', r'\\bgreetings\\b']\n",
    "    if any(re.search(pattern, email_body.lower()) for pattern in greeting_patterns):\n",
    "        score += 1\n",
    "    else:\n",
    "        issues.append(\"Missing proper greeting\")\n",
    "\n",
    "    # 3. Check email has closing/signature (1 point)\n",
    "    closing_patterns = [r'\\bbest\\b', r'\\bregards\\b', r'\\bthanks\\b', r'\\bsincerely\\b']\n",
    "    if any(re.search(pattern, email_body.lower()) for pattern in closing_patterns):\n",
    "        score += 1\n",
    "    else:\n",
    "        issues.append(\"Missing proper closing\")\n",
    "\n",
    "    # 4. Check reasonable email length (1 point)\n",
    "    word_count = len(email_body.split())\n",
    "    if 50 <= word_count <= 400:\n",
    "        score += 1\n",
    "    else:\n",
    "        issues.append(f\"Email length inappropriate: {word_count} words\")\n",
    "\n",
    "    # 5. Check proper sentence structure (1 point)\n",
    "    sentences = email_body.split('.')\n",
    "    proper_sentences = sum(1 for s in sentences if s.strip() and s.strip()[0].isupper())\n",
    "    if proper_sentences >= len([s for s in sentences if s.strip()]) * 0.8:\n",
    "        score += 1\n",
    "    else:\n",
    "        issues.append(\"Poor sentence capitalization\")\n",
    "\n",
    "    normalized_score = score / max_score\n",
    "\n",
    "    return Feedback(value=normalized_score, rationale=f\"Email format compliance: {score}/{max_score} points. Issues: {'; '.join(issues) if issues else 'None'}\")\n",
    "\n",
    "\n",
    "# A Scorer operates on a MLflow Trace, so let's retrieve a few Traces:\n",
    "print('\\nüîç Loading recent traces from our email demo app...')\n",
    "\n",
    "# Load recent traces for evaluation\n",
    "traces = mlflow.search_traces(\n",
    "    max_results=3,\n",
    "    filter_string='status = \"OK\"',\n",
    "    order_by=['timestamp DESC'],\n",
    ")\n",
    "print(f\"‚úÖ Found {len(traces)} traces for evaluation\")\n",
    "\n",
    "# Now, let's run evaluation using this scorer\n",
    "\n",
    "eval_results = mlflow.genai.evaluate(data=traces, scorers=[email_format_compliance])\n",
    "\n",
    "print(f\"\\nüìä Evaluation completed!\")\n",
    "print(f\"üÜî Run ID: {eval_results._run_id}\")\n",
    "\n",
    "# Generate and display evaluation links\n",
    "generate_evaluation_links(eval_results._run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Complete MLflow Evaluation Mastery - Summary & Next Steps\n",
    "\n",
    "Congratulations! You've successfully mastered MLflow's complete evaluation ecosystem for GenAI applications.\n",
    "\n",
    "## What You've Accomplished\n",
    "\n",
    "‚úÖ **Understood the Architecture** - Judges vs Scorers and how they work together  \n",
    "‚úÖ **Explored Direct Judge Calls** - Tested individual judges with sample inputs  \n",
    "‚úÖ **Used Predefined Scorers** - Applied research-backed judges with default data extraction  \n",
    "‚úÖ **Created Hybrid Approaches** - Combined proven judges with custom data extraction  \n",
    "‚úÖ **Built Guidelines-based Judges** - Encoded business requirements in natural language  \n",
    "‚úÖ **Implemented Prompt-based Judges** - Created sophisticated custom evaluation logic  \n",
    "‚úÖ **Developed Code-based Scorers** - Built fast, deterministic evaluation functions\n",
    "\n",
    "## Resources for Continued Learning\n",
    "\n",
    "üìö **MLflow Documentation**\n",
    "\n",
    "- [**Complete Judge & Scorer Reference**](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/) - Full API documentation\n",
    "- [**Evaluation Best Practices**](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/evaluate-app) - Production deployment guide\n",
    "- [**Monitoring & Alerting**](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/monitor-app) - Continuous quality monitoring\n",
    "\n",
    "üéØ **Remember**: Great evaluation is iterative. Start with the basics, learn from your results, and progressively build more sophisticated assessment as your understanding deepens.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow_genai_email_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
