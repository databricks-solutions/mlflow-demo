{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Monitoring: Automated Quality at Scale\n",
    "\n",
    "MLflow's production monitoring automatically runs quality assessments on a sample of your production traffic, ensuring your GenAI app maintains high quality standards without manual intervention. MLflow lets you use the **same** metrics you defined for offline evaluation in production, enabling you to have consistent quality evaluation across your entire application lifecycle - dev to prod.\n",
    "\n",
    "Key benefits:\n",
    "\n",
    "- **Automated evaluation** - Run LLM judges on production traces with configurable sampling rates\n",
    "- **Continuous quality assessment** - Monitor quality metrics in real-time without disrupting user experience\n",
    "- **Cost-effective monitoring** - Smart sampling strategies to balance coverage with computational cost\n",
    "\n",
    "Production monitoring enables you to deploy confidently, knowing that you will proactively detect issues so you can address them before they cause a major impact to your users.\n",
    "\n",
    "![monitoring-overview](https://i.imgur.com/wv4p562.gif)\n",
    "\n",
    "This notebook demonstrates how to take the quality metrics you created in development and turn them into production monitoring with MLflow's new scorer registration system. You'll learn how the same scorers that work offline for evaluation automatically become online monitoring - no rebuilding required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages (only required if running in a Databricks Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -r ../../requirements.txt\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Load environment variables and verify MLflow configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import mlflow\n",
    "from mlflow_demo.utils import *\n",
    "\n",
    "if mlflow.utils.databricks_utils.is_in_databricks_notebook():\n",
    "  print(\"Running in Databricks Notebook\")\n",
    "  setup_databricks_notebook_env()\n",
    "else:\n",
    "  print(\"Running in Local IDE\")\n",
    "  setup_local_ide_env()\n",
    "\n",
    "# Verify key variables are loaded\n",
    "print('=== Environment Setup ===')\n",
    "print(f'DATABRICKS_HOST: {os.getenv(\"DATABRICKS_HOST\")}')\n",
    "print(f'MLFLOW_EXPERIMENT_ID: {os.getenv(\"MLFLOW_EXPERIMENT_ID\")}')\n",
    "print(f'LLM_MODEL: {os.getenv(\"LLM_MODEL\")}')\n",
    "print(f'UC_CATALOG: {os.getenv(\"UC_CATALOG\")}')\n",
    "print(f'UC_SCHEMA: {os.getenv(\"UC_SCHEMA\")}')\n",
    "print('‚úÖ Environment variables loaded successfully!')\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get helper functions for showing links to generated traces\n",
    "from mlflow_demo.utils import generate_trace_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ From Offline Evaluation to Online Monitoring\n",
    "\n",
    "The key insight of MLflow's production monitoring is simple: **the same scorers you use for offline evaluation automatically become your online monitoring**. No need to rebuild, reconfigure, or rewrite your quality assessment logic.\n",
    "\n",
    "## The \"Same Metrics Everywhere\" Approach\n",
    "\n",
    "- **Development**: Use Guidelines, Safety, and custom scorers to evaluate quality\n",
    "- **Testing**: Run the same scorers on evaluation datasets\n",
    "- **Production**: Register those exact scorers for automated monitoring\n",
    "- **Analysis**: Compare quality using consistent metrics across all stages\n",
    "\n",
    "## From Notebook 2 to Production\n",
    "\n",
    "In this tutorial, we'll take the scorers we created in **Notebook 2: Create Quality Metrics** and turn them into production monitoring in just a few lines of code.\n",
    "\n",
    "**üìö Documentation**\n",
    "\n",
    "- [**Run Scorers in Production**](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/run-scorer-in-prod) - Production monitoring setup\n",
    "\n",
    "**‚ñ∂Ô∏è Run the next cells to recreate and register your quality metrics for production monitoring!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù Step 1: Register Quality Scorers for Monitoring\n",
    "\n",
    "Let's recreate the exact same scorers from Notebook 2 and register them for production monitoring. This demonstrates how your development metrics seamlessly become production monitoring.\n",
    "\n",
    "**‚ñ∂Ô∏è Run the next cells to register your scorers!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import scorer\n",
    "from mlflow.genai.scorers import Guidelines\n",
    "\n",
    "# Copy / paste the exact same scorers from Notebook 2: Create Quality Metrics\n",
    "\n",
    "# Tone of voice Guideline - Ensure professional tone\n",
    "tone = Guidelines(\n",
    "  name='tone',\n",
    "  guidelines=\"\"\"The response maintains a professional tone.\"\"\")\n",
    "\n",
    "# Accuracy Guideline - Verify all facts come from provided data\n",
    "@scorer\n",
    "def accuracy(trace):\n",
    "    \"\"\"\n",
    "    Custom accuracy scorer that evaluates only the email body content,\n",
    "    excluding the subject line to avoid false negatives on creative/generic subjects.\n",
    "\n",
    "    This demonstrates how to wrap the proven Guidelines judge with custom data extraction.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from mlflow.genai.judges import meets_guidelines\n",
    "    # Extract the original request\n",
    "    outputs = json.loads(trace.data.response)\n",
    "    email_body = outputs.get('email_body')\n",
    "    user_input = outputs.get('user_input')\n",
    "    input_facts = trace.search_spans(span_type=\"RETRIEVER\")[0].outputs\n",
    "\n",
    "    accuracy_guideline = \"\"\"The email_body correctly references all factual information from the provided_info based on these rules:\n",
    "- All factual information must be directly sourced from the provided data with NO fabrication\n",
    "- Names, dates, numbers, and company details must be 100% accurate with no errors\n",
    "- Meeting discussions must be summarized with the exact same sentiment and priority as presented in the data\n",
    "- Support ticket information must include correct ticket IDs, status, and resolution details when available\n",
    "- All product usage statistics must be presented with the same metrics provided in the data\n",
    "- No references to CloudFlow features, services, or offerings unless specifically mentioned in the customer data\n",
    "- AUTOMATIC FAIL if any information is mentioned that is not explicitly provided in the data\n",
    "- It is OK if the email_body follows the user_input request to omit certain facts, as long as no fabricated facts are introduced.\"\"\"\n",
    "\n",
    "    # Use the proven Guidelines judge with our extracted email body\n",
    "    return meets_guidelines(guidelines=accuracy_guideline, context={'provided_info': input_facts, 'email': email_body, 'user_input': user_input})\n",
    "# Personalization Guideline - Ensure emails are tailored to specific customers\n",
    "@scorer\n",
    "def personalized(trace):\n",
    "    \"\"\"\n",
    "    Custom personalization scorer that evaluates only the email body content,\n",
    "    excluding the subject line to avoid false negatives on creative/generic subjects.\n",
    "\n",
    "    This demonstrates how to wrap the proven Guidelines judge with custom data extraction.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from mlflow.genai.judges import meets_guidelines\n",
    "    # Extract the original request\n",
    "    outputs = json.loads(trace.data.response)\n",
    "    email_body = outputs.get('email_body')\n",
    "    user_input = outputs.get('user_input')\n",
    "    input_facts = trace.search_spans(span_type=\"RETRIEVER\")[0].outputs\n",
    "\n",
    "    personalized_guideline = \"\"\"The email_body demonstrates clear personalization based on the provided_info based on these rules:\n",
    "- Email must begin by referencing the most recent meeting/interaction\n",
    "- Immediately next, the email must address the customer's MOST pressing concern as evidenced in the data\n",
    "- Content structure must be customized based on the account's health status (critical issues first for \"Fair\" or \"Poor\" accounts)\n",
    "- Industry-specific language must be used that reflects the customer's sector\n",
    "- Recommendations must ONLY reference features that are:\n",
    "  a) Listed as \"least_used_features\" in the data, AND\n",
    "  b) Directly related to the \"potential_opportunity\" field\n",
    "- Relationship history must be acknowledged (new vs. mature relationship)\n",
    "- Deal stage must influence communication approach (implementation vs. renewal vs. growth)\n",
    "- AUTOMATIC FAIL if recommendations could be copied to another customer in a different situation\"\"\"\n",
    "\n",
    "    # Use the proven Guidelines judge with our extracted email body\n",
    "    return meets_guidelines(guidelines=personalized_guideline, context={'provided_info': input_facts, 'email': email_body, 'user_input': user_input})\n",
    "\n",
    "# Relevance Guideline - Prioritize content by urgency\n",
    "@scorer\n",
    "def relevance(trace):\n",
    "    \"\"\"\n",
    "    Custom relevance scorer that evaluates only the email body content,\n",
    "    excluding the subject line to avoid false negatives on creative/generic subjects.\n",
    "\n",
    "    This demonstrates how to wrap the proven Guidelines judge with custom data extraction.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from mlflow.genai.judges import meets_guidelines\n",
    "    # Extract the original request\n",
    "    outputs = json.loads(trace.data.response)\n",
    "    email_body = outputs.get('email_body')\n",
    "    user_input = outputs.get('user_input')\n",
    "    input_facts = trace.search_spans(span_type=\"RETRIEVER\")[0].outputs\n",
    "\n",
    "    relevance_guideline = \"\"\"The email_body prioritizes content that matters to the recipient in the provided_info based on these rules:\n",
    "- Critical support tickets (status=\"Open (Critical)\") must be addressed after the greeting, reference to the most recent interaction, any pleasantries, and references to closed tickets\n",
    "- Time-sensitive action items must be addressed before general updates\n",
    "- Content must be ordered by descending urgency as defined by:\n",
    "  1. Critical support issues\n",
    "  2. Action items explicitly stated in most recent meeting\n",
    "  3. Upcoming renewal if within 30 days\n",
    "  4. Recently resolved issues\n",
    "  5. Usage trends and recommendations\n",
    "- No more than ONE feature recommendation for accounts with open critical issues\n",
    "- No mentions of company news, product releases, or success stories not directly requested by the customer\n",
    "- No calls to action unrelated to the immediate needs in the data\n",
    "- AUTOMATIC FAIL if the email requests a meeting without being tied to a specific action item or opportunity in the data\"\"\"\n",
    "\n",
    "    # Use the proven Guidelines judge with our extracted email body\n",
    "    return meets_guidelines(guidelines=relevance_guideline, context={'provided_info': input_facts, 'email': email_body, 'user_input': user_input})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we remove all existing registered scorers since the demo by default registers & starts monitoring for the same scorers as we will use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import list_scorers, delete_scorer\n",
    "\n",
    "registered_scorers = list_scorers()\n",
    "for s in registered_scorers:\n",
    "    print(f'Deleting existing registered scorer: {s.name}')\n",
    "    delete_scorer(name=s.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will register and start the scorers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import ScorerSamplingConfig\n",
    "\n",
    "print('üöÄ Registering and starting production monitoring scorers...')\n",
    "\n",
    "tone.register()\n",
    "tone.start(sampling_config=ScorerSamplingConfig(sample_rate=1)) # run on 100% of productions traces\n",
    "print('‚úÖ Tone scorer registered and started!')\n",
    "\n",
    "accuracy.register()\n",
    "accuracy.start(sampling_config=ScorerSamplingConfig(sample_rate=1)) # run on 100% of productions traces\n",
    "print('‚úÖ Accuracy scorer registered and started!')\n",
    "\n",
    "personalized.register()\n",
    "personalized.start(sampling_config=ScorerSamplingConfig(sample_rate=1)) # run on 100% of productions traces\n",
    "print('‚úÖ Personalized scorer registered and started!')\n",
    "\n",
    "relevance.register()\n",
    "relevance.start(sampling_config=ScorerSamplingConfig(sample_rate=1)) # run on 100% of productions traces\n",
    "print('‚úÖ Relevance scorer registered and started!')\n",
    "\n",
    "print('üéØ All production monitoring scorers are now active!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Step 2: View Monitoring Results in MLflow UI\n",
    "\n",
    "Let's issue a few queries so we can see the monitoring results in the MLflow Trace UI.  Since monitoring runs every ~15 minutes, please wait ~20 mins after running these queries to check the UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow_demo.agent.email_generator import EmailGenerator\n",
    "\n",
    "email_agent = EmailGenerator()\n",
    "\n",
    "\n",
    "result = email_agent.generate_email_with_retrieval(\"EcomSolutions LLC\", user_input=\"Include a product recommendation to improve their usage.\")\n",
    "generate_trace_links(result['trace_id']);\n",
    "\n",
    "result = email_agent.generate_email_with_retrieval(\"LogiTrans Solutions\", user_input=\"Talk in all caps!\")\n",
    "generate_trace_links(result['trace_id']);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully implemented production monitoring for GenAI applications using MLflow 3.x.\n",
    "\n",
    "## What You've Accomplished\n",
    "\n",
    "‚úÖ **Connected offline to online** - Used the same scorers from development in production monitoring  \n",
    "‚úÖ **Registered quality scorers** - Set up 5 scorers for automated monitoring  \n",
    "\n",
    "**üìö Continue Learning**\n",
    "\n",
    "- [Production Monitoring Guide](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/run-scorer-in-prod) - Complete setup documentation\n",
    "- [MLflow GenAI Evaluation](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/) - Full evaluation and monitoring workflow\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow_genai_email_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
